{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDRL Phase 3 - Global Coordinator with Hierarchical Deep Reinforcement Learning\n",
    "\n",
    "## AWS Multi-Region Deployment (us-east-1 + eu-west-1)\n",
    "\n",
    "**Architecture:**\n",
    "- Global Coordinator: High-level task allocation using Actor-Critic\n",
    "- Local PPO Agents: Pre-trained from Phase 2 (AWS-US-EAST-1 + AWS-EU-WEST-1)\n",
    "- Task Segmentation: K-means clustering for complexity-based allocation\n",
    "- Privacy: Differential Privacy for inter-agent communication\n",
    "\n",
    "**Version:** 1.0\n",
    "**Date:** November 2025\n",
    "**Status:** Phase 3 - Hierarchical Coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds\n",
    "GLOBAL_SEED = 42\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "print(\"✅ Standard imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML/RL framework imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tf.random.set_seed(GLOBAL_SEED)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(f\"✅ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"✅ NumPy version: {np.__version__}\")\n",
    "print(f\"✅ Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "BASE_PATH = '/content/drive/MyDrive/mythesis/srilakshmi/HDRL_Research'\n",
    "DATA_PATH = os.path.join(BASE_PATH, 'data')\n",
    "MODEL_PATH = os.path.join(BASE_PATH, 'models')\n",
    "RESULTS_PATH = os.path.join(BASE_PATH, 'results')\n",
    "\n",
    "os.makedirs(os.path.join(MODEL_PATH, 'global_coordinator'), exist_ok=True)\n",
    "os.makedirs(os.path.join(RESULTS_PATH, 'phase3_aws'), exist_ok=True)\n",
    "\n",
    "print(f\"✅ Base path: {BASE_PATH}\")\n",
    "print(f\"✅ Directories verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'processed', 'train_tasks.csv'))\n",
    "val_df = pd.read_csv(os.path.join(DATA_PATH, 'processed', 'val_tasks.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATA_PATH, 'processed', 'test_tasks.csv'))\n",
    "\n",
    "print(f\"✅ Training data loaded: {len(train_df)} tasks\")\n",
    "print(f\"✅ Validation data loaded: {len(val_df)} tasks\")\n",
    "print(f\"✅ Test data loaded: {len(test_df)} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Phase 2 Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Task Segmentation Module from Phase 2\n",
    "with open(os.path.join(MODEL_PATH, 'task_segmenter_v3.pkl'), 'rb') as f:\n",
    "    task_segmenter = pickle.load(f)\n",
    "\n",
    "print(f\"✅ Task segmenter loaded: {task_segmenter.n_segments} segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Differential Privacy Configuration\n",
    "with open(os.path.join(MODEL_PATH, 'dp_layer_config_v3.json'), 'r') as f:\n",
    "    dp_config = json.load(f)\n",
    "\n",
    "print(f\"✅ DP config loaded: ε={dp_config['epsilon']}, δ={dp_config['delta']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Phase 2 training history\n",
    "with open(os.path.join(RESULTS_PATH, 'phase2_v3_aws/training_history.pkl'), 'rb') as f:\n",
    "    phase2_history = pickle.load(f)\n",
    "\n",
    "print(\"✅ Phase 2 training history loaded:\")\n",
    "for region in phase2_history.keys():\n",
    "    avg_reward = np.mean(phase2_history[region]['episode_rewards'][-10:])\n",
    "    print(f\"  {region}: Final avg reward = {avg_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cloud Provider Configuration (Same as Phase 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudProviderConfig:\n",
    "    \"\"\"AWS cloud provider configuration\"\"\"\n",
    "    \n",
    "    def __init__(self, name, region, cpu_capacity, memory_capacity, storage_capacity,\n",
    "                 cost_per_cpu_hour, cost_per_gb_hour, energy_per_cpu_hour,\n",
    "                 energy_per_gb_hour, base_latency, pricing_model):\n",
    "        self.name = name\n",
    "        self.region = region\n",
    "        self.cpu_capacity = cpu_capacity\n",
    "        self.memory_capacity = memory_capacity\n",
    "        self.storage_capacity = storage_capacity\n",
    "        self.cost_per_cpu_hour = cost_per_cpu_hour\n",
    "        self.cost_per_gb_hour = cost_per_gb_hour\n",
    "        self.energy_per_cpu_hour = energy_per_cpu_hour\n",
    "        self.energy_per_gb_hour = energy_per_gb_hour\n",
    "        self.base_latency = base_latency\n",
    "        self.pricing_model = pricing_model\n",
    "        self.current_cpu_used = 0\n",
    "        self.current_memory_used = 0\n",
    "        self.current_storage_used = 0\n",
    "    \n",
    "    def get_available_resources(self):\n",
    "        return {\n",
    "            'cpu_available': self.cpu_capacity - self.current_cpu_used,\n",
    "            'memory_available': self.memory_capacity - self.current_memory_used,\n",
    "            'storage_available': self.storage_capacity - self.current_storage_used\n",
    "        }\n",
    "    \n",
    "    def allocate_resources(self, cpu, memory, storage):\n",
    "        self.current_cpu_used += cpu\n",
    "        self.current_memory_used += memory\n",
    "        self.current_storage_used += storage\n",
    "    \n",
    "    def release_resources(self, cpu, memory, storage):\n",
    "        self.current_cpu_used = max(0, self.current_cpu_used - cpu)\n",
    "        self.current_memory_used = max(0, self.current_memory_used - memory)\n",
    "        self.current_storage_used = max(0, self.current_storage_used - storage)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_cpu_used = 0\n",
    "        self.current_memory_used = 0\n",
    "        self.current_storage_used = 0\n",
    "\n",
    "# Initialize TWO AWS providers\n",
    "providers = {\n",
    "    'AWS-US-EAST-1': CloudProviderConfig(\n",
    "        name='AWS-US-EAST-1',\n",
    "        region='us-east-1',\n",
    "        cpu_capacity=1000,\n",
    "        memory_capacity=4000,\n",
    "        storage_capacity=10000,\n",
    "        cost_per_cpu_hour=0.05,\n",
    "        cost_per_gb_hour=0.01,\n",
    "        energy_per_cpu_hour=2.5,\n",
    "        energy_per_gb_hour=0.5,\n",
    "        base_latency=10,\n",
    "        pricing_model='On-Demand'\n",
    "    ),\n",
    "    'AWS-EU-WEST-1': CloudProviderConfig(\n",
    "        name='AWS-EU-WEST-1',\n",
    "        region='eu-west-1',\n",
    "        cpu_capacity=1000,\n",
    "        memory_capacity=4000,\n",
    "        storage_capacity=10000,\n",
    "        cost_per_cpu_hour=0.015,\n",
    "        cost_per_gb_hour=0.003,\n",
    "        energy_per_cpu_hour=2.3,\n",
    "        energy_per_gb_hour=0.48,\n",
    "        base_latency=90,\n",
    "        pricing_model='Spot'\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"✅ AWS providers configured (2 regions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Differential Privacy Layer (Enhanced for Communication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentialPrivacyLayer:\n",
    "    def __init__(self, epsilon=1.0, delta=1e-5, sensitivity=1.0):\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        self.sensitivity = sensitivity\n",
    "        self.noise_scale = self._calculate_noise_scale()\n",
    "        self.privacy_budget_used = 0.0\n",
    "        self.communication_count = 0\n",
    "    \n",
    "    def _calculate_noise_scale(self):\n",
    "        import math\n",
    "        sigma = math.sqrt(2 * math.log(1.25 / self.delta)) * self.sensitivity / self.epsilon\n",
    "        return sigma\n",
    "    \n",
    "    def add_noise(self, data, sensitivity=None):\n",
    "        if sensitivity is None:\n",
    "            sensitivity = self.sensitivity\n",
    "        noise_scale = self.noise_scale * sensitivity\n",
    "        noise = np.random.normal(0, noise_scale, size=data.shape)\n",
    "        self.privacy_budget_used += self.epsilon\n",
    "        self.communication_count += 1\n",
    "        return (data + noise).astype(np.float32)\n",
    "    \n",
    "    def get_privacy_budget(self):\n",
    "        return {\n",
    "            'epsilon': self.epsilon,\n",
    "            'delta': self.delta,\n",
    "            'budget_used': self.privacy_budget_used,\n",
    "            'communications': self.communication_count\n",
    "        }\n",
    "    \n",
    "    def reset_budget(self):\n",
    "        self.privacy_budget_used = 0.0\n",
    "        self.communication_count = 0\n",
    "\n",
    "dp_layer = DifferentialPrivacyLayer(\n",
    "    epsilon=dp_config['epsilon'],\n",
    "    delta=dp_config['delta'],\n",
    "    sensitivity=1.0\n",
    ")\n",
    "\n",
    "print(f\"✅ DP layer initialized: ε={dp_layer.epsilon}, δ={dp_layer.delta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Cloud Environment (Reuse from Phase 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiCloudEnvironment:\n",
    "    def __init__(self, provider, max_steps=200, time_step=60):\n",
    "        self.provider = provider\n",
    "        self.max_steps = max_steps\n",
    "        self.time_step = time_step\n",
    "        self.current_step = 0\n",
    "        self.current_time = 0\n",
    "        self.task_queue = deque()\n",
    "        self.running_tasks = []\n",
    "        self.completed_tasks = []\n",
    "        self.failed_tasks = []\n",
    "        self.total_cost = 0\n",
    "        self.total_energy = 0\n",
    "        self.total_waiting_time = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.current_time = 0\n",
    "        self.task_queue.clear()\n",
    "        self.running_tasks.clear()\n",
    "        self.completed_tasks.clear()\n",
    "        self.failed_tasks.clear()\n",
    "        self.total_cost = 0\n",
    "        self.total_energy = 0\n",
    "        self.total_waiting_time = 0\n",
    "        self.provider.reset()\n",
    "        return self._get_state()\n",
    "    \n",
    "    def add_task(self, task):\n",
    "        self.task_queue.append(task)\n",
    "    \n",
    "    def _process_completed_tasks(self):\n",
    "        completed = []\n",
    "        for task_info in self.running_tasks:\n",
    "            if task_info['completion_time'] <= self.current_time:\n",
    "                self.provider.release_resources(\n",
    "                    task_info['cpu'], task_info['mem'], task_info['storage']\n",
    "                )\n",
    "                self.completed_tasks.append(task_info['task'])\n",
    "                completed.append(task_info)\n",
    "        \n",
    "        for task_info in completed:\n",
    "            self.running_tasks.remove(task_info)\n",
    "        \n",
    "        return len(completed)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.current_time += self.time_step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        num_completed = self._process_completed_tasks()\n",
    "        done = self.current_step >= self.max_steps\n",
    "        \n",
    "        if len(self.task_queue) == 0:\n",
    "            return self._get_state(), -1, done, {'num_completed': num_completed}\n",
    "        \n",
    "        max_selection = min(50, len(self.task_queue))\n",
    "        task_idx = min(action, max_selection - 1)\n",
    "        task_list = list(self.task_queue)\n",
    "        selected_task = task_list[task_idx]\n",
    "        self.task_queue.remove(selected_task)\n",
    "        \n",
    "        cpu_req = selected_task.get('cpu_request', 0.5)\n",
    "        mem_req = selected_task.get('memory_request', 1.0)\n",
    "        storage_req = selected_task.get('data_size', 0.1)\n",
    "        duration = selected_task.get('duration', 60)\n",
    "        arrival_time = selected_task.get('timestamp', 0)\n",
    "        \n",
    "        waiting_time = max(0, self.current_time - arrival_time)\n",
    "        self.total_waiting_time += waiting_time\n",
    "        \n",
    "        resources = self.provider.get_available_resources()\n",
    "        \n",
    "        if (cpu_req <= resources['cpu_available'] and\n",
    "            mem_req <= resources['memory_available']):\n",
    "            \n",
    "            self.provider.allocate_resources(cpu_req, mem_req, storage_req)\n",
    "            \n",
    "            cost = (cpu_req * self.provider.cost_per_cpu_hour * duration / 3600 +\n",
    "                   mem_req * self.provider.cost_per_gb_hour * duration / 3600)\n",
    "            self.total_cost += cost\n",
    "            \n",
    "            energy = (cpu_req * self.provider.energy_per_cpu_hour * duration / 3600 +\n",
    "                     mem_req * self.provider.energy_per_gb_hour * duration / 3600)\n",
    "            self.total_energy += energy\n",
    "            \n",
    "            latency = self.provider.base_latency + (duration / 1000.0)\n",
    "            \n",
    "            completion_time = self.current_time + duration\n",
    "            self.running_tasks.append({\n",
    "                'task': selected_task,\n",
    "                'completion_time': completion_time,\n",
    "                'cpu': cpu_req,\n",
    "                'mem': mem_req,\n",
    "                'storage': storage_req\n",
    "            })\n",
    "            \n",
    "            reward = self._calculate_reward(\n",
    "                cpu=cpu_req, mem=mem_req, cost=cost, energy=energy,\n",
    "                latency=latency, duration=duration,\n",
    "                waiting_time=waiting_time, success=True\n",
    "            )\n",
    "            \n",
    "            info = {\n",
    "                'success': True, 'cost': cost, 'energy': energy,\n",
    "                'latency': latency, 'waiting_time': waiting_time,\n",
    "                'num_completed': num_completed,\n",
    "                'queue_length': len(self.task_queue),\n",
    "                'num_running': len(self.running_tasks),\n",
    "                'task_idx': task_idx\n",
    "            }\n",
    "        else:\n",
    "            self.failed_tasks.append(selected_task)\n",
    "            reward = -10\n",
    "            info = {\n",
    "                'success': False, 'num_completed': num_completed,\n",
    "                'queue_length': len(self.task_queue),\n",
    "                'num_running': len(self.running_tasks),\n",
    "                'task_idx': task_idx\n",
    "            }\n",
    "        \n",
    "        return self._get_state(), reward, done, info\n",
    "    \n",
    "    def _calculate_reward(self, cpu, mem, cost, energy, latency, duration, waiting_time, success):\n",
    "        if not success:\n",
    "            return -10\n",
    "        \n",
    "        cpu_util = self.provider.current_cpu_used / self.provider.cpu_capacity\n",
    "        mem_util = self.provider.current_memory_used / self.provider.memory_capacity\n",
    "        avg_util = (cpu_util + mem_util) / 2\n",
    "        \n",
    "        if 0.6 <= avg_util <= 0.8:\n",
    "            utilization_reward = 10\n",
    "        elif avg_util > 0.8:\n",
    "            utilization_reward = 10 - (avg_util - 0.8) * 30\n",
    "        else:\n",
    "            utilization_reward = avg_util * 12\n",
    "        \n",
    "        queue_length = len(self.task_queue)\n",
    "        if queue_length < 10:\n",
    "            queue_reward = 5\n",
    "        elif queue_length < 30:\n",
    "            queue_reward = 2\n",
    "        else:\n",
    "            queue_reward = -(queue_length - 30) * 0.2\n",
    "        \n",
    "        max_acceptable_wait = 300\n",
    "        waiting_penalty = -(waiting_time - max_acceptable_wait) * 0.01 if waiting_time > max_acceptable_wait else 0\n",
    "        \n",
    "        task_size = cpu * mem * duration\n",
    "        cost_efficiency = -cost / (task_size + 1e-6) * 5\n",
    "        \n",
    "        energy_penalty = -energy * 0.05\n",
    "        completion_bonus = 3\n",
    "        \n",
    "        reward = (\n",
    "            0.30 * utilization_reward +\n",
    "            0.20 * queue_reward +\n",
    "            0.15 * waiting_penalty +\n",
    "            0.15 * cost_efficiency +\n",
    "            0.10 * energy_penalty +\n",
    "            0.10 * completion_bonus\n",
    "        )\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _get_state(self):\n",
    "        resources = self.provider.get_available_resources()\n",
    "        cpu_avail = resources['cpu_available'] / self.provider.cpu_capacity\n",
    "        mem_avail = resources['memory_available'] / self.provider.memory_capacity\n",
    "        cpu_util = self.provider.current_cpu_used / self.provider.cpu_capacity\n",
    "        mem_util = self.provider.current_memory_used / self.provider.memory_capacity\n",
    "        \n",
    "        queue_length_norm = min(len(self.task_queue) / 100.0, 1.0)\n",
    "        num_running_norm = min(len(self.running_tasks) / 50.0, 1.0)\n",
    "        \n",
    "        if len(self.task_queue) > 0:\n",
    "            queue_tasks = list(self.task_queue)[:10]\n",
    "            avg_queue_cpu = np.mean([t.get('cpu_request', 0.5) for t in queue_tasks])\n",
    "            avg_queue_mem = np.mean([t.get('memory_request', 1.0) for t in queue_tasks])\n",
    "            avg_queue_priority = np.mean([t.get('priority', 0) for t in queue_tasks])\n",
    "        else:\n",
    "            avg_queue_cpu = avg_queue_mem = avg_queue_priority = 0\n",
    "        \n",
    "        cost_norm = self.provider.cost_per_cpu_hour / 0.5\n",
    "        energy_norm = self.provider.energy_per_cpu_hour / 5.0\n",
    "        latency_norm = self.provider.base_latency / 200.0\n",
    "        \n",
    "        step_progress = self.current_step / self.max_steps\n",
    "        completion_rate = len(self.completed_tasks) / max(self.current_step, 1)\n",
    "        failure_rate = len(self.failed_tasks) / max(self.current_step, 1)\n",
    "        cost_so_far_norm = min(self.total_cost / 1000.0, 1.0)\n",
    "        \n",
    "        state = np.array([\n",
    "            cpu_avail, mem_avail, cpu_util, mem_util,\n",
    "            queue_length_norm, num_running_norm,\n",
    "            avg_queue_cpu, avg_queue_mem, avg_queue_priority,\n",
    "            cost_norm, energy_norm, latency_norm,\n",
    "            step_progress, completion_rate, failure_rate, cost_so_far_norm,\n",
    "            0.0, 0.0, 0.0, 0.0\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return state\n",
    "\n",
    "print(\"✅ MultiCloudEnvironment defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Local PPO Agents (Load Pre-trained from Phase 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_DIM = 20\n",
    "ACTION_DIM = 50\n",
    "\n",
    "class PPOActorCritic(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim, hidden_units=[256, 128]):\n",
    "        super(PPOActorCritic, self).__init__()\n",
    "        self.dense1 = layers.Dense(hidden_units[0], activation='relu')\n",
    "        self.dense2 = layers.Dense(hidden_units[1], activation='relu')\n",
    "        self.dropout = layers.Dropout(0.2)\n",
    "        self.actor = layers.Dense(action_dim)\n",
    "        self.critic = layers.Dense(1)\n",
    "    \n",
    "    def call(self, state, training=False):\n",
    "        x = self.dense1(state)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.actor(x), self.critic(x)\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        if len(state.shape) == 1:\n",
    "            state = tf.expand_dims(state, 0)\n",
    "        action_logits, value = self(state, training=training)\n",
    "        dist = tfp.distributions.Categorical(logits=action_logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.numpy()[0], log_prob.numpy()[0], value.numpy()[0][0]\n",
    "\n",
    "print(\"✅ PPO Actor-Critic class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained local agents from Phase 2\n",
    "local_agents = {}\n",
    "environments = {}\n",
    "\n",
    "for provider_name, provider_config in providers.items():\n",
    "    # Create agent\n",
    "    agent = PPOActorCritic(state_dim=STATE_DIM, action_dim=ACTION_DIM)\n",
    "    \n",
    "    # Initialize with dummy forward pass\n",
    "    dummy_state = tf.random.normal((1, STATE_DIM))\n",
    "    _ = agent(dummy_state)\n",
    "    \n",
    "    # Load weights from Phase 2\n",
    "    weights_path = os.path.join(MODEL_PATH, f'ppo_agents_v3_aws/{provider_name}_final.weights.h5')\n",
    "    agent.load_weights(weights_path)\n",
    "    \n",
    "    local_agents[provider_name] = agent\n",
    "    \n",
    "    # Create environment\n",
    "    env = MultiCloudEnvironment(provider_config, max_steps=200, time_step=60)\n",
    "    environments[provider_name] = env\n",
    "    \n",
    "    print(f\"✅ {provider_name}: Pre-trained agent loaded\")\n",
    "\n",
    "print(f\"\\n✅ Total local agents: {len(local_agents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Global Coordinator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Coordinator input: aggregated states from 2 local agents\n",
    "# Each local state: 20-dim → Total: 40-dim\n",
    "# Plus task features: 6-dim (cpu, mem, duration, priority, complexity, segment)\n",
    "GLOBAL_STATE_DIM = 40 + 6\n",
    "\n",
    "# Global Coordinator action: allocate task to region 0 or 1\n",
    "GLOBAL_ACTION_DIM = 2  # Binary decision: US-EAST-1 or EU-WEST-1\n",
    "\n",
    "class GlobalCoordinator(tf.keras.Model):\n",
    "    \"\"\"Global Coordinator using Actor-Critic for high-level task allocation\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_units=[256, 128, 64]):\n",
    "        super(GlobalCoordinator, self).__init__()\n",
    "        self.dense1 = layers.Dense(hidden_units[0], activation='relu')\n",
    "        self.dense2 = layers.Dense(hidden_units[1], activation='relu')\n",
    "        self.dense3 = layers.Dense(hidden_units[2], activation='relu')\n",
    "        self.dropout1 = layers.Dropout(0.2)\n",
    "        self.dropout2 = layers.Dropout(0.2)\n",
    "        \n",
    "        # Actor: outputs region allocation probabilities\n",
    "        self.actor = layers.Dense(action_dim)\n",
    "        \n",
    "        # Critic: estimates value of current global state\n",
    "        self.critic = layers.Dense(1)\n",
    "    \n",
    "    def call(self, state, training=False):\n",
    "        x = self.dense1(state)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.dense3(x)\n",
    "        return self.actor(x), self.critic(x)\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"Select which region to allocate task to\"\"\"\n",
    "        if len(state.shape) == 1:\n",
    "            state = tf.expand_dims(state, 0)\n",
    "        action_logits, value = self(state, training=training)\n",
    "        dist = tfp.distributions.Categorical(logits=action_logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.numpy()[0], log_prob.numpy()[0], value.numpy()[0][0]\n",
    "    \n",
    "    def evaluate(self, states, actions):\n",
    "        action_logits, values = self(states)\n",
    "        dist = tfp.distributions.Categorical(logits=action_logits)\n",
    "        return dist.log_prob(actions), values, dist.entropy()\n",
    "\n",
    "# Initialize Global Coordinator\n",
    "global_coordinator = GlobalCoordinator(\n",
    "    state_dim=GLOBAL_STATE_DIM,\n",
    "    action_dim=GLOBAL_ACTION_DIM,\n",
    "    hidden_units=[256, 128, 64]\n",
    ")\n",
    "\n",
    "print(f\"✅ Global Coordinator initialized:\")\n",
    "print(f\"   Input: {GLOBAL_STATE_DIM}-dim (2 agents × 20-dim + 6-dim task features)\")\n",
    "print(f\"   Output: {GLOBAL_ACTION_DIM} regions (US-EAST-1 or EU-WEST-1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Global Coordinator Trainer (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalCoordinatorTrainer:\n",
    "    \"\"\"PPO trainer for Global Coordinator\"\"\"\n",
    "    \n",
    "    def __init__(self, model, learning_rate=3e-4, gamma=0.99, lam=0.95,\n",
    "                 clip_ratio=0.2, value_coef=0.5, entropy_coef=0.01):\n",
    "        self.model = model\n",
    "        self.optimizer = Adam(learning_rate=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.last_policy_loss = 0\n",
    "        self.last_value_loss = 0\n",
    "    \n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            next_value = 0 if t == len(rewards) - 1 else values[t + 1]\n",
    "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.lam * (1 - dones[t]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        returns = [adv + val for adv, val in zip(advantages, values)]\n",
    "        return np.array(advantages, dtype=np.float32), np.array(returns, dtype=np.float32)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, states, actions, old_log_probs, advantages, returns):\n",
    "        with tf.GradientTape() as tape:\n",
    "            log_probs, values, entropy = self.model.evaluate(states, actions)\n",
    "            values = tf.squeeze(values)\n",
    "            ratio = tf.exp(log_probs - old_log_probs)\n",
    "            clipped_ratio = tf.clip_by_value(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio)\n",
    "            policy_loss = -tf.reduce_mean(tf.minimum(\n",
    "                ratio * advantages, clipped_ratio * advantages\n",
    "            ))\n",
    "            value_loss = tf.reduce_mean(tf.square(returns - values))\n",
    "            entropy_loss = -tf.reduce_mean(entropy)\n",
    "            total_loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss\n",
    "        gradients = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        return policy_loss, value_loss, entropy_loss\n",
    "    \n",
    "    def update(self, trajectory, epochs=10, batch_size=64):\n",
    "        states = np.array(trajectory['states'])\n",
    "        actions = np.array(trajectory['actions'])\n",
    "        old_log_probs = np.array(trajectory['log_probs'])\n",
    "        rewards = np.array(trajectory['rewards'])\n",
    "        values = np.array(trajectory['values'])\n",
    "        dones = np.array(trajectory['dones'])\n",
    "        \n",
    "        advantages, returns = self.compute_gae(rewards, values, dones)\n",
    "        advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)\n",
    "        \n",
    "        states = tf.constant(states, dtype=tf.float32)\n",
    "        actions = tf.constant(actions, dtype=tf.int32)\n",
    "        old_log_probs = tf.constant(old_log_probs, dtype=tf.float32)\n",
    "        advantages = tf.constant(advantages, dtype=tf.float32)\n",
    "        returns = tf.constant(returns, dtype=tf.float32)\n",
    "        \n",
    "        dataset_size = len(states)\n",
    "        indices = np.arange(dataset_size)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, dataset_size, batch_size):\n",
    "                end = min(start + batch_size, dataset_size)\n",
    "                batch_indices = indices[start:end]\n",
    "                policy_loss, value_loss, _ = self.train_step(\n",
    "                    tf.gather(states, batch_indices),\n",
    "                    tf.gather(actions, batch_indices),\n",
    "                    tf.gather(old_log_probs, batch_indices),\n",
    "                    tf.gather(advantages, batch_indices),\n",
    "                    tf.gather(returns, batch_indices)\n",
    "                )\n",
    "        \n",
    "        self.last_policy_loss = policy_loss.numpy()\n",
    "        self.last_value_loss = value_loss.numpy()\n",
    "\n",
    "gc_trainer = GlobalCoordinatorTrainer(global_coordinator, learning_rate=3e-4)\n",
    "print(\"✅ Global Coordinator Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hierarchical Communication Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_local_states(environments, dp_layer):\n",
    "    \"\"\"\n",
    "    Aggregate states from all local agents with differential privacy.\n",
    "    Returns: (40-dim) concatenated state vector\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    region_names = list(environments.keys())\n",
    "    \n",
    "    for region_name in region_names:\n",
    "        env = environments[region_name]\n",
    "        local_state = env._get_state()  # 20-dim\n",
    "        \n",
    "        # Apply differential privacy to state before sending to Global Coordinator\n",
    "        privatized_state = dp_layer.add_noise(local_state, sensitivity=0.1)\n",
    "        states.append(privatized_state)\n",
    "    \n",
    "    # Concatenate states from all regions\n",
    "    aggregated_state = np.concatenate(states)  # 40-dim for 2 regions\n",
    "    return aggregated_state, region_names\n",
    "\n",
    "def extract_task_features(task, task_segmenter):\n",
    "    \"\"\"\n",
    "    Extract features from a task for Global Coordinator.\n",
    "    Returns: (6-dim) task feature vector\n",
    "    \"\"\"\n",
    "    cpu = task.get('cpu_request', 0.5)\n",
    "    mem = task.get('memory_request', 1.0)\n",
    "    duration = task.get('duration', 60)\n",
    "    priority = task.get('priority', 0)\n",
    "    \n",
    "    # Calculate complexity using task segmenter\n",
    "    complexity = task_segmenter.calculate_complexity_score(task)\n",
    "    segment = task_segmenter.predict_segment(task)\n",
    "    \n",
    "    # Normalize features\n",
    "    task_features = np.array([\n",
    "        cpu / 10.0,  # Normalize\n",
    "        mem / 20.0,\n",
    "        duration / 3600.0,\n",
    "        priority / 10.0,\n",
    "        complexity,\n",
    "        segment / 5.0\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    return task_features\n",
    "\n",
    "def create_global_state(aggregated_local_states, task_features):\n",
    "    \"\"\"\n",
    "    Create global state for Global Coordinator.\n",
    "    Returns: (46-dim) state = 40-dim (local states) + 6-dim (task features)\n",
    "    \"\"\"\n",
    "    global_state = np.concatenate([aggregated_local_states, task_features])\n",
    "    return global_state\n",
    "\n",
    "print(\"✅ Hierarchical communication protocol defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Workload Generation (Same as Phase 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_workload(n_tasks, base_data, random_seed=None):\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    samples = base_data.sample(n=n_tasks, replace=True if len(base_data) < n_tasks else False, random_state=random_seed)\n",
    "    workload = []\n",
    "    for idx, row in samples.iterrows():\n",
    "        task = {\n",
    "            'task_id': f'task_{idx}_{random_seed if random_seed else 0}',\n",
    "            'timestamp': row.get('timestamp', 0) + np.random.uniform(-3600, 3600),\n",
    "            'cpu_request': max(0.1, row.get('cpu_request', 0.5) * np.random.uniform(0.8, 1.2)),\n",
    "            'memory_request': max(0.1, row.get('memory_request', 1.0) * np.random.uniform(0.8, 1.2)),\n",
    "            'duration': max(10, row.get('duration', 60) * np.random.uniform(0.7, 1.3)),\n",
    "            'priority': row.get('priority', 0),\n",
    "            'data_size': max(0.01, row.get('data_size', 0.1) * np.random.uniform(0.9, 1.1)),\n",
    "            'task_type': row.get('task_type', 0),\n",
    "            'has_dependency': row.get('has_dependency', 0),\n",
    "            'resource_intensity': row.get('resource_intensity', 0.5)\n",
    "        }\n",
    "        workload.append(task)\n",
    "    return workload\n",
    "\n",
    "print(\"✅ Workload generation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Phase 3 Hierarchical Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 50\n",
    "MAX_STEPS_PER_EPISODE = 200\n",
    "NUM_TASKS = 5000\n",
    "SAVE_INTERVAL = 10\n",
    "\n",
    "# Training history\n",
    "training_history = {\n",
    "    'global': {\n",
    "        'episode_rewards': [],\n",
    "        'total_costs': [],\n",
    "        'total_energy': [],\n",
    "        'privacy_budget_used': [],\n",
    "        'region_allocations': []  # Track which regions were selected\n",
    "    }\n",
    "}\n",
    "\n",
    "for provider_name in providers.keys():\n",
    "    training_history[provider_name] = {\n",
    "        'episode_rewards': [],\n",
    "        'episode_costs': [],\n",
    "        'completed_tasks': [],\n",
    "        'failed_tasks': []\n",
    "    }\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"STARTING PHASE 3 HIERARCHICAL TRAINING - {NUM_EPISODES} EPISODES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Regions: {list(providers.keys())}\")\n",
    "print(f\"Global Coordinator: {GLOBAL_STATE_DIM}-dim input, {GLOBAL_ACTION_DIM} actions\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"\\nEPISODE {episode + 1}/{NUM_EPISODES}\")\n",
    "    \n",
    "    # Generate workload\n",
    "    workload = generate_synthetic_workload(\n",
    "        n_tasks=NUM_TASKS,\n",
    "        base_data=train_df.sample(min(1000, len(train_df)), random_state=episode),\n",
    "        random_seed=episode\n",
    "    )\n",
    "    \n",
    "    # Reset all environments\n",
    "    for env in environments.values():\n",
    "        env.reset()\n",
    "    \n",
    "    # Reset DP budget for this episode\n",
    "    dp_layer.reset_budget()\n",
    "    \n",
    "    # Global Coordinator trajectory\n",
    "    gc_trajectory = {\n",
    "        'states': [],\n",
    "        'actions': [],\n",
    "        'log_probs': [],\n",
    "        'rewards': [],\n",
    "        'values': [],\n",
    "        'dones': []\n",
    "    }\n",
    "    \n",
    "    episode_total_cost = 0\n",
    "    episode_total_energy = 0\n",
    "    episode_gc_reward = 0\n",
    "    region_allocation_counts = {name: 0 for name in providers.keys()}\n",
    "    region_names = list(providers.keys())\n",
    "    \n",
    "    # Process tasks one by one through Global Coordinator\n",
    "    for step, task in enumerate(workload[:MAX_STEPS_PER_EPISODE]):\n",
    "        # 1. Aggregate states from local agents (with DP)\n",
    "        aggregated_states, _ = aggregate_local_states(environments, dp_layer)\n",
    "        \n",
    "        # 2. Extract task features\n",
    "        task_features = extract_task_features(task, task_segmenter)\n",
    "        \n",
    "        # 3. Create global state\n",
    "        global_state = create_global_state(aggregated_states, task_features)\n",
    "        \n",
    "        # 4. Global Coordinator selects region\n",
    "        region_action, log_prob, value = global_coordinator.get_action(global_state, training=True)\n",
    "        selected_region = region_names[region_action]\n",
    "        region_allocation_counts[selected_region] += 1\n",
    "        \n",
    "        # 5. Add task to selected region's environment\n",
    "        selected_env = environments[selected_region]\n",
    "        selected_env.add_task(task)\n",
    "        \n",
    "        # 6. Local agent executes task\n",
    "        local_agent = local_agents[selected_region]\n",
    "        local_state = selected_env._get_state()\n",
    "        privatized_local_state = dp_layer.add_noise(local_state, sensitivity=0.1)\n",
    "        local_action, _, _ = local_agent.get_action(privatized_local_state, training=False)  # Use pre-trained weights\n",
    "        \n",
    "        # 7. Environment step\n",
    "        next_state, local_reward, done, info = selected_env.step(local_action)\n",
    "        \n",
    "        # 8. Calculate global reward (sum of rewards from both regions)\n",
    "        global_reward = local_reward\n",
    "        \n",
    "        # Add cost/energy efficiency bonus to global reward\n",
    "        if info.get('success', False):\n",
    "            cost = info['cost']\n",
    "            energy = info['energy']\n",
    "            episode_total_cost += cost\n",
    "            episode_total_energy += energy\n",
    "            \n",
    "            # Bonus for choosing cheaper region (EU-WEST-1)\n",
    "            if selected_region == 'AWS-EU-WEST-1':\n",
    "                global_reward += 2  # Bonus for using spot instances\n",
    "        \n",
    "        episode_gc_reward += global_reward\n",
    "        \n",
    "        # 9. Store trajectory\n",
    "        gc_trajectory['states'].append(global_state)\n",
    "        gc_trajectory['actions'].append(region_action)\n",
    "        gc_trajectory['log_probs'].append(log_prob)\n",
    "        gc_trajectory['rewards'].append(global_reward)\n",
    "        gc_trajectory['values'].append(value)\n",
    "        gc_trajectory['dones'].append(done)\n",
    "        \n",
    "        if done or step >= MAX_STEPS_PER_EPISODE - 1:\n",
    "            break\n",
    "    \n",
    "    # 10. Update Global Coordinator\n",
    "    if len(gc_trajectory['states']) >= 32:\n",
    "        gc_trainer.update(gc_trajectory, epochs=10, batch_size=64)\n",
    "    \n",
    "    # 11. Record episode metrics\n",
    "    training_history['global']['episode_rewards'].append(episode_gc_reward)\n",
    "    training_history['global']['total_costs'].append(episode_total_cost)\n",
    "    training_history['global']['total_energy'].append(episode_total_energy)\n",
    "    training_history['global']['privacy_budget_used'].append(dp_layer.privacy_budget_used)\n",
    "    training_history['global']['region_allocations'].append(region_allocation_counts)\n",
    "    \n",
    "    for region_name, env in environments.items():\n",
    "        training_history[region_name]['episode_costs'].append(env.total_cost)\n",
    "        training_history[region_name]['completed_tasks'].append(len(env.completed_tasks))\n",
    "        training_history[region_name]['failed_tasks'].append(len(env.failed_tasks))\n",
    "    \n",
    "    # 12. Print episode summary\n",
    "    print(f\"  Global Reward: {episode_gc_reward:.2f}\")\n",
    "    print(f\"  Total Cost: ${episode_total_cost:.2f}\")\n",
    "    print(f\"  Privacy Budget Used: ε={dp_layer.privacy_budget_used:.2f}\")\n",
    "    print(f\"  Region Allocation: US-EAST={region_allocation_counts['AWS-US-EAST-1']}, EU-WEST={region_allocation_counts['AWS-EU-WEST-1']}\")\n",
    "    for region_name, env in environments.items():\n",
    "        print(f\"  [{region_name}] Completed: {len(env.completed_tasks)}, Failed: {len(env.failed_tasks)}, Cost: ${env.total_cost:.2f}\")\n",
    "    \n",
    "    # 13. Save checkpoints\n",
    "    if (episode + 1) % SAVE_INTERVAL == 0:\n",
    "        global_coordinator.save_weights(\n",
    "            os.path.join(MODEL_PATH, f'global_coordinator/gc_ep{episode + 1}.weights.h5')\n",
    "        )\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 3 HIERARCHICAL TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Global Coordinator\n",
    "global_coordinator.save_weights(\n",
    "    os.path.join(MODEL_PATH, 'global_coordinator/gc_final.weights.h5')\n",
    ")\n",
    "\n",
    "# Save training history\n",
    "with open(os.path.join(RESULTS_PATH, 'phase3_aws/training_history.pkl'), 'wb') as f:\n",
    "    pickle.dump(training_history, f)\n",
    "\n",
    "# Calculate statistics\n",
    "stats = {\n",
    "    'global': {\n",
    "        'avg_reward': np.mean(training_history['global']['episode_rewards']),\n",
    "        'avg_total_cost': np.mean(training_history['global']['total_costs']),\n",
    "        'avg_total_energy': np.mean(training_history['global']['total_energy']),\n",
    "        'avg_privacy_budget': np.mean(training_history['global']['privacy_budget_used'])\n",
    "    }\n",
    "}\n",
    "\n",
    "for region_name in providers.keys():\n",
    "    stats[region_name] = {\n",
    "        'avg_cost': np.mean(training_history[region_name]['episode_costs']),\n",
    "        'avg_completed': np.mean(training_history[region_name]['completed_tasks']),\n",
    "        'avg_failed': np.mean(training_history[region_name]['failed_tasks'])\n",
    "    }\n",
    "\n",
    "with open(os.path.join(RESULTS_PATH, 'phase3_aws/training_stats.json'), 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(\"✅ Models and results saved\")\n",
    "print(\"\\nGlobal Coordinator Stats:\")\n",
    "print(f\"  Avg Reward: {stats['global']['avg_reward']:.2f}\")\n",
    "print(f\"  Avg Total Cost: ${stats['global']['avg_total_cost']:.2f}\")\n",
    "print(f\"  Avg Privacy Budget: ε={stats['global']['avg_privacy_budget']:.2f}\")\n",
    "\n",
    "print(\"\\nRegional Stats:\")\n",
    "for region_name in providers.keys():\n",
    "    print(f\"{region_name}:\")\n",
    "    print(f\"  Avg Cost: ${stats[region_name]['avg_cost']:.2f}\")\n",
    "    print(f\"  Avg Completed: {stats[region_name]['avg_completed']:.0f} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "fig.suptitle('Phase 3: Global Coordinator Training Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Global Coordinator Rewards\n",
    "axes[0, 0].plot(training_history['global']['episode_rewards'], color='purple', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Global Reward')\n",
    "axes[0, 0].set_title('Global Coordinator Rewards (Should INCREASE)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Total Costs\n",
    "axes[0, 1].plot(training_history['global']['total_costs'], color='red', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Total Cost ($)')\n",
    "axes[0, 1].set_title('Total System Cost (Should DECREASE)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Privacy Budget Usage\n",
    "axes[1, 0].plot(training_history['global']['privacy_budget_used'], color='green', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Privacy Budget (ε)')\n",
    "axes[1, 0].set_title('Privacy Budget Usage per Episode')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Region Allocation Distribution\n",
    "us_allocations = [alloc['AWS-US-EAST-1'] for alloc in training_history['global']['region_allocations']]\n",
    "eu_allocations = [alloc['AWS-EU-WEST-1'] for alloc in training_history['global']['region_allocations']]\n",
    "axes[1, 1].plot(us_allocations, label='AWS-US-EAST-1 (On-Demand)', color='blue')\n",
    "axes[1, 1].plot(eu_allocations, label='AWS-EU-WEST-1 (Spot)', color='orange')\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Tasks Allocated')\n",
    "axes[1, 1].set_title('Region Allocation by Global Coordinator')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Completed Tasks per Region\n",
    "for region_name in providers.keys():\n",
    "    axes[2, 0].plot(training_history[region_name]['completed_tasks'], label=region_name)\n",
    "axes[2, 0].set_xlabel('Episode')\n",
    "axes[2, 0].set_ylabel('Completed Tasks')\n",
    "axes[2, 0].set_title('Task Completion by Region')\n",
    "axes[2, 0].legend()\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Cost per Region\n",
    "for region_name in providers.keys():\n",
    "    axes[2, 1].plot(training_history[region_name]['episode_costs'], label=region_name)\n",
    "axes[2, 1].set_xlabel('Episode')\n",
    "axes[2, 1].set_ylabel('Cost ($)')\n",
    "axes[2, 1].set_title('Cost per Region')\n",
    "axes[2, 1].legend()\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_PATH, 'phase3_aws/training_viz.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Learning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PHASE 3 LEARNING CURVE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Global Coordinator\n",
    "gc_rewards = training_history['global']['episode_rewards']\n",
    "gc_costs = training_history['global']['total_costs']\n",
    "\n",
    "early_reward = np.mean(gc_rewards[:10])\n",
    "late_reward = np.mean(gc_rewards[-10:])\n",
    "early_cost = np.mean(gc_costs[:10])\n",
    "late_cost = np.mean(gc_costs[-10:])\n",
    "\n",
    "improvement = late_reward - early_reward\n",
    "cost_reduction = early_cost - late_cost\n",
    "\n",
    "print(\"\\nGlobal Coordinator:\")\n",
    "print(f\"  Reward: {early_reward:.2f} → {late_reward:.2f} ({improvement:+.2f})\")\n",
    "print(f\"  Total Cost: ${early_cost:.2f} → ${late_cost:.2f} (${cost_reduction:+.2f})\")\n",
    "\n",
    "if improvement > 0 and cost_reduction > 0:\n",
    "    print(f\"  ✅ HIERARCHICAL LEARNING SUCCESSFUL!\")\n",
    "elif improvement > 0:\n",
    "    print(f\"  ⚠️ PARTIAL LEARNING (reward improved but not cost)\")\n",
    "else:\n",
    "    print(f\"  ❌ NO LEARNING DETECTED\")\n",
    "\n",
    "# Region allocation analysis\n",
    "early_alloc = training_history['global']['region_allocations'][:10]\n",
    "late_alloc = training_history['global']['region_allocations'][-10:]\n",
    "\n",
    "early_eu_pct = np.mean([a['AWS-EU-WEST-1'] / (a['AWS-US-EAST-1'] + a['AWS-EU-WEST-1']) for a in early_alloc])\n",
    "late_eu_pct = np.mean([a['AWS-EU-WEST-1'] / (a['AWS-US-EAST-1'] + a['AWS-EU-WEST-1']) for a in late_alloc])\n",
    "\n",
    "print(\"\\nRegion Allocation Learning:\")\n",
    "print(f\"  EU-WEST-1 (Spot) usage: {early_eu_pct*100:.1f}% → {late_eu_pct*100:.1f}%\")\n",
    "if late_eu_pct > early_eu_pct:\n",
    "    print(f\"  ✅ Coordinator learned to prefer cheaper region!\")\n",
    "else:\n",
    "    print(f\"  ⚠️ Coordinator did not strongly prefer cheaper region\")\n",
    "\n",
    "# Privacy budget\n",
    "avg_privacy_budget = np.mean(training_history['global']['privacy_budget_used'])\n",
    "print(f\"\\nPrivacy Analysis:\")\n",
    "print(f\"  Avg privacy budget per episode: ε={avg_privacy_budget:.2f}\")\n",
    "print(f\"  Total privacy budget (composition): ε={avg_privacy_budget * NUM_EPISODES:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Comparison: Phase 2 vs Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PHASE 2 vs PHASE 3 COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate Phase 2 total costs\n",
    "phase2_us_cost = np.mean(phase2_history['AWS-US-EAST-1']['episode_costs'])\n",
    "phase2_eu_cost = np.mean(phase2_history['AWS-EU-WEST-1']['episode_costs'])\n",
    "phase2_total_cost = phase2_us_cost + phase2_eu_cost\n",
    "\n",
    "# Calculate Phase 3 total cost\n",
    "phase3_total_cost = np.mean(training_history['global']['total_costs'])\n",
    "\n",
    "# Calculate Phase 2 total rewards\n",
    "phase2_us_reward = np.mean(phase2_history['AWS-US-EAST-1']['episode_rewards'])\n",
    "phase2_eu_reward = np.mean(phase2_history['AWS-EU-WEST-1']['episode_rewards'])\n",
    "phase2_total_reward = phase2_us_reward + phase2_eu_reward\n",
    "\n",
    "# Phase 3 total reward\n",
    "phase3_total_reward = np.mean(training_history['global']['episode_rewards'])\n",
    "\n",
    "print(\"\\nCost Comparison:\")\n",
    "print(f\"  Phase 2 (Independent Agents): ${phase2_total_cost:.2f}\")\n",
    "print(f\"  Phase 3 (Global Coordinator): ${phase3_total_cost:.2f}\")\n",
    "cost_improvement = ((phase2_total_cost - phase3_total_cost) / phase2_total_cost) * 100\n",
    "print(f\"  Improvement: {cost_improvement:+.2f}%\")\n",
    "\n",
    "print(\"\\nReward Comparison:\")\n",
    "print(f\"  Phase 2 (Independent Agents): {phase2_total_reward:.2f}\")\n",
    "print(f\"  Phase 3 (Global Coordinator): {phase3_total_reward:.2f}\")\n",
    "reward_improvement = phase3_total_reward - phase2_total_reward\n",
    "print(f\"  Improvement: {reward_improvement:+.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 3 IMPLEMENTATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n✅ Global Coordinator trained with hierarchical RL\")\n",
    "print(\"✅ Task segmentation integrated (K-means)\")\n",
    "print(\"✅ Differential privacy applied to communication\")\n",
    "print(\"✅ Multi-region coordination (US-EAST-1 + EU-WEST-1)\")\n",
    "print(\"\\nNext: Deploy to real AWS infrastructure and evaluate!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
